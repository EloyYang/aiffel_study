{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#뉴스기사 크롤링 분류"
      ],
      "metadata": {
        "id": "D9QzNEcSl9p3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZfJx-QtgTN8"
      },
      "outputs": [],
      "source": [
        "# 크롤러를 만들기 전 필요한 도구들을 임포트\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "\n",
        "# 페이지 수, 카테고리, 날짜를 입력\n",
        "def make_urllist(page_num, code, date):\n",
        "  urllist= []\n",
        "  for i in range(1, page_num + 1):\n",
        "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}\n",
        "    news = requests.get(url, headers=headers)\n",
        "\n",
        "    # BeautifulSoup의 인스턴스 생성\n",
        "    soup = BeautifulSoup(news.content, 'html.parser')\n",
        "\n",
        "    # CASE 1\n",
        "    news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
        "    # CASE 2\n",
        "    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
        "\n",
        "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'출력\n",
        "    for line in news_list:\n",
        "        urllist.append(line.a.get('href'))\n",
        "  return urllist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_list = []"
      ],
      "metadata": {
        "id": "8Lp26VLfiGBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#코드 네이밍\n",
        "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}"
      ],
      "metadata": {
        "id": "8SOTkrSDiGHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#- 데이터프레임을 생성\n",
        "def make_data(urllist, code):\n",
        "  text_list = []\n",
        "  for url in urllist:\n",
        "    article = Article(url, language='ko')\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    text_list.append(article.title)\n",
        "\n",
        "  #- 데이터프레임의 'news' 키 아래 파싱한 텍스트를 밸류\n",
        "  df = pd.DataFrame({'news': text_list})\n",
        "\n",
        "  #- 데이터프레임의 'code' 키 아래 한글 카테고리명\n",
        "  df['code'] = idx2word[str(code)]\n",
        "  return df"
      ],
      "metadata": {
        "id": "q9-XOiE3iGDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_list = [101, 102, 103, 105]\n",
        "\n",
        "code_list"
      ],
      "metadata": {
        "id": "fG4-1EBPiGJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_total_data(page_num, code_list, date):\n",
        "  df = None\n",
        "\n",
        "  for code in code_list:\n",
        "    url_list = make_urllist(page_num, code, date)\n",
        "    df_temp = make_data(url_list, code)\n",
        "    print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
        "\n",
        "    if df is not None:\n",
        "      df = pd.concat([df, df_temp])\n",
        "    else:\n",
        "      df = df_temp\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "YbWMjIu3iGLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = make_total_data(1, code_list, 20231128)"
      ],
      "metadata": {
        "id": "b1UMcS5YiGPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#다른 일자 데이터 추가\n",
        "df = df.append(make_total_data(1, code_list, 20231123))"
      ],
      "metadata": {
        "id": "W4duiOzCiGRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 데이터프레임 파일을 csv 파일로 저장\n",
        "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "if os.path.exists(csv_path):\n",
        "  print('{} File Saved!'.format(csv_path))"
      ],
      "metadata": {
        "id": "44w5odqPiGUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
        "df = pd.read_table(csv_path, sep=',')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6arKhGfLiGWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규 표현식을 이용해서 한글 외의 문자 제거\n",
        "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "df['news']"
      ],
      "metadata": {
        "id": "NuSA09q1iGX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Null 값 확인\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "b9uAvq5siGZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복된 샘플들을 제거합니다.\n",
        "df.drop_duplicates(subset=['news'], inplace=True)\n",
        "\n",
        "print('뉴스 기사의 개수: ',len(df))"
      ],
      "metadata": {
        "id": "2gZqa-MdiGbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
        "\n",
        "df['code'].value_counts().plot(kind = 'bar')"
      ],
      "metadata": {
        "id": "Pvl7avYAiGeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.groupby('code').size().reset_index(name = 'count'))"
      ],
      "metadata": {
        "id": "5DzeJnGQiGfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "tokenizer = Mecab()\n",
        "\n",
        "#- kor_text에 csv 파일 내용 넣기\n",
        "kor_text = df['news'].to_string(index=False)\n",
        "\n",
        "#- 형태소 분석, 즉 토큰화(tokenization)를 합니다.\n",
        "print(tokenizer.morphs(kor_text))"
      ],
      "metadata": {
        "id": "ymvXTVnWiGj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스', '도',  '인', '서', '올', '와', '내일’, ‘형’, ‘로’, ‘시대’, ’곳', '과', '또', '하는', '하', '는', '속' ,'제' ,'도' ,'덜' ,'본', '부터', '전', '지역', '새', '까지', ]"
      ],
      "metadata": {
        "id": "6aLIw1a7iGmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#불용어 개수 확인\n",
        "print(len(stopwords))"
      ],
      "metadata": {
        "id": "6i7Y2vR0iGqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수입니다.\n",
        "def preprocessing(data):\n",
        "  text_data = []\n",
        "\n",
        "  for sentence in data:\n",
        "    temp_data = []\n",
        "    #- 토큰화\n",
        "    temp_data = tokenizer.morphs(sentence)\n",
        "    #- 불용어 제거\n",
        "    temp_data = [word for word in temp_data if not word in stopwords]\n",
        "    text_data.append(temp_data)\n",
        "\n",
        "  text_data = list(map(' '.join, text_data))\n",
        "\n",
        "  return text_data"
      ],
      "metadata": {
        "id": "BXVqCVR2iGsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = preprocessing(df['news'])\n",
        "print(text_data[0])"
      ],
      "metadata": {
        "id": "cYMUY3eLiGub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "EamY0mIgiGwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#- 훈련 데이터와 테스트 데이터를 분리합니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)"
      ],
      "metadata": {
        "id": "AOVZ7A_KiG2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#- 훈련 데이터와 테스트 데이터를 분리합니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)"
      ],
      "metadata": {
        "id": "ZkTXHTawjuJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#- 훈련 데이터와 테스트 데이터를 분리합니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)"
      ],
      "metadata": {
        "id": "2H5nOw9CjuLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "\n",
        "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "\n",
        "#- 나이브 베이즈 분류기를 수행합니다.\n",
        "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
        "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "e-sXyqmpjuNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_vectorizer(data):\n",
        "  data_counts = count_vect.transform(data)\n",
        "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
        "  return data_tfidf"
      ],
      "metadata": {
        "id": "a_XcnD7MjuPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "6qq595gvjuRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#회고\n",
        "\n",
        "처음 접하는 내용들이라 노드의 내용을 이해하는데 시간을 많이 쏟았지만\n",
        "이해가 되지 않더라도 바로 하나씩 직접 실습해보면서 이해를 했으면 더 빠르게 좋은 결과를 만들어 내지 않았을까 하는 아쉬움이 많았습니다.\n"
      ],
      "metadata": {
        "id": "YZtD4UMqkrBz"
      }
    }
  ]
}